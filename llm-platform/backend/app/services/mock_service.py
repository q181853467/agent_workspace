from typing import AsyncGenerator
import asyncio
import time
import uuid
from app.services.model_service import BaseModelService
from app.schemas.chat import (
    ChatCompletionRequest, ChatCompletionResponse, ChatCompletionChunk,
    ChatCompletionChoice, ChatCompletionUsage, ChatMessage, MessageRole,
    ChatCompletionChunkChoice, ChatCompletionChunkDelta
)
from app.models.model import Model

class MockModelService(BaseModelService):
    """Mock model service for demonstration purposes"""
    
    def __init__(self):
        self.mock_responses = {
            "gpt-4": {
                "base": "æˆ‘æ˜¯GPT-4ï¼ŒOpenAIå¼€å‘çš„æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹ã€‚æˆ‘æ‹¥æœ‰å¼ºå¤§çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥å¤„ç†å¤æ‚çš„å¤šæ­¥éª¤é—®é¢˜ã€‚",
                "capabilities": ["ç¼–ç¨‹", "æ•°å­¦", "å†™ä½œ", "åˆ†æ", "åˆ›æ„", "ç¿»è¯‘", "æ€»ç»“"],
                "response_time": (1.0, 3.0)  # å“åº”æ—¶é—´èŒƒå›´ï¼ˆç§’ï¼‰
            },
            "gpt-3.5-turbo": {
                "base": "æˆ‘æ˜¯GPT-3.5 Turboï¼Œå¿«é€Ÿé«˜æ•ˆçš„AIåŠ©æ‰‹ã€‚æˆ‘åœ¨ä¿æŒé«˜è´¨é‡è¾“å‡ºçš„åŒæ—¶ï¼Œèƒ½å¤Ÿå¿«é€Ÿå“åº”å„ç§è¯¢é—®ã€‚",
                "capabilities": ["å¯¹è¯", "é—®ç­”", "æ–‡æœ¬å¤„ç†", "ç®€å•ç¼–ç¨‹", "æ€»ç»“"],
                "response_time": (0.5, 1.5)
            },
            "deepseek-coder": {
                "base": "æˆ‘æ˜¯DeepSeek Coderï¼Œä¸“é—¨ä¸ºç¼–ç¨‹ä»»åŠ¡ä¼˜åŒ–çš„AIæ¨¡å‹ã€‚æˆ‘åœ¨ä»£ç ç”Ÿæˆã€è°ƒè¯•å’Œç®—æ³•è®¾è®¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚",
                "capabilities": ["ç¼–ç¨‹", "è°ƒè¯•", "ä»£ç å®¡æŸ¥", "ç®—æ³•è®¾è®¡", "æ¶æ„è®¾è®¡"],
                "response_time": (0.8, 2.0)
            },
            "deepseek-chat": {
                "base": "æˆ‘æ˜¯DeepSeek Chatï¼Œç”±æ·±åº¦æ±‚ç´¢å¼€å‘çš„å¯¹è¯AIæ¨¡å‹ã€‚æˆ‘ä¸“æ³¨äºä¸­æ–‡å¯¹è¯ï¼Œç†è§£ä¸­å›½æ–‡åŒ–èƒŒæ™¯ã€‚",
                "capabilities": ["ä¸­æ–‡å¯¹è¯", "æ–‡å­¦åˆ›ä½œ", "å¤è¯—è¯", "å†å²æ–‡åŒ–", "ç”Ÿæ´»å»ºè®®"],
                "response_time": (0.6, 1.8)
            },
            "claude-3": {
                "base": "æˆ‘æ˜¯Claude 3ï¼ŒAnthropicå¼€å‘çš„AIåŠ©æ‰‹ã€‚æˆ‘è‡´åŠ›äºæä¾›å®‰å…¨ã€æœ‰ç”¨ã€è¯šå®çš„å¸®åŠ©ï¼Œæ³¨é‡ä¼¦ç†å’Œå®‰å…¨æ€§ã€‚",
                "capabilities": ["åˆ†æ", "å†™ä½œ", "æ€»ç»“", "æ¨ç†", "é“å¾·åˆ¤æ–­", "åˆ›æ„"],
                "response_time": (1.2, 2.5)
            }
        }
        
        # æ™ºèƒ½å›å¤æ¨¡æ¿
        self.response_templates = {
            "greeting": [
                "æ‚¨å¥½ï¼å¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è§£å†³å„ç§é—®é¢˜ï¼Œè¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ã€‚",
                "æ¬¢è¿ä½¿ç”¨æˆ‘ä»¬çš„AIæœåŠ¡ï¼æˆ‘æ˜¯æ‚¨çš„æ™ºèƒ½åŠ©æ‰‹ï¼Œéšæ—¶å‡†å¤‡ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚",
                "æ‚¨å¥½ï¼æˆ‘æ˜¯æ‚¨çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å¤„ç†å„ç§ä»»åŠ¡ã€‚è¯·é—®ä»Šå¤©æˆ‘èƒ½ä¸ºæ‚¨åšäº›ä»€ä¹ˆï¼Ÿ"
            ],
            "programming": [
                "ä½œä¸ºç¼–ç¨‹ä¸“å®¶ï¼Œæˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š\n1. ç¼–å†™å’Œä¼˜åŒ–ä»£ç \n2. è°ƒè¯•ç¨‹åºé”™è¯¯\n3. è®¾è®¡ç®—æ³•å’Œæ•°æ®ç»“æ„\n4. ä»£ç å®¡æŸ¥å’Œé‡æ„\n\nè¯·æè¿°æ‚¨çš„å…·ä½“éœ€æ±‚ã€‚",
                "æˆ‘å¾ˆä¹æ„å¸®åŠ©æ‚¨è§£å†³ç¼–ç¨‹é—®é¢˜ï¼æ— è®ºæ˜¯è¯­æ³•é”™è¯¯ã€é€»è¾‘é—®é¢˜è¿˜æ˜¯æ¶æ„è®¾è®¡ï¼Œæˆ‘éƒ½èƒ½æä¾›ä¸“ä¸šå»ºè®®ã€‚",
                "ç¼–ç¨‹æ˜¯æˆ‘çš„å¼ºé¡¹ï¼æˆ‘å¯ä»¥æ”¯æŒå¤šç§ç¼–ç¨‹è¯­è¨€ï¼ŒåŒ…æ‹¬Pythonã€JavaScriptã€Javaã€C++ç­‰ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨é‡åˆ°çš„å…·ä½“é—®é¢˜ã€‚"
            ],
            "analysis": [
                "æˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›æ·±å…¥çš„åˆ†æï¼ŒåŒ…æ‹¬ï¼š\nâ€¢ æ•°æ®åˆ†æå’Œå¯è§†åŒ–\nâ€¢ è¶‹åŠ¿é¢„æµ‹\nâ€¢ é—®é¢˜æ ¹å› åˆ†æ\nâ€¢ æ–¹æ¡ˆå¯¹æ¯”è¯„ä¼°\n\nè¯·åˆ†äº«æ‚¨éœ€è¦åˆ†æçš„å†…å®¹ã€‚",
                "åˆ†æå’Œæ¨ç†æ˜¯æˆ‘çš„æ ¸å¿ƒèƒ½åŠ›ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ä»ä¸åŒè§’åº¦æ€è€ƒé—®é¢˜ï¼Œæä¾›å®¢è§‚çš„è§è§£å’Œå»ºè®®ã€‚",
                "è®©æˆ‘ä¸ºæ‚¨è¿›è¡Œä¸“ä¸šåˆ†æã€‚æˆ‘ä¼šè¿ç”¨é€»è¾‘æ¨ç†å’Œæ•°æ®åˆ†ææ–¹æ³•ï¼Œä¸ºæ‚¨æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚"
            ],
            "creative": [
                "åˆ›æ„å·¥ä½œæ˜¯æˆ‘å–œæ¬¢çš„æŒ‘æˆ˜ï¼æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š\nâœ¨ åˆ›æ„å†™ä½œå’Œæ•…äº‹åˆ›ä½œ\nğŸ¨ è®¾è®¡æ–¹æ¡ˆå’Œæ¦‚å¿µå¼€å‘\nğŸ’¡ å¤´è„‘é£æš´å’Œåˆ›æ„æ¿€å‘\nğŸ“ æ–‡æ¡ˆç­–åˆ’å’Œå†…å®¹åˆ›ä½œ",
                "æˆ‘å¾ˆå…´å¥‹èƒ½å‚ä¸æ‚¨çš„åˆ›æ„é¡¹ç›®ï¼æ— è®ºæ˜¯è‰ºæœ¯åˆ›ä½œã€æ–‡å­¦å†™ä½œè¿˜æ˜¯å•†ä¸šåˆ›æ„ï¼Œæˆ‘éƒ½èƒ½æä¾›ç‹¬ç‰¹çš„è§†è§’ã€‚",
                "åˆ›æ„æ— é™ï¼æˆ‘å¯ä»¥ä»å¤šä¸ªç»´åº¦ä¸ºæ‚¨çš„é¡¹ç›®æ³¨å…¥æ–°çš„æƒ³æ³•å’Œçµæ„Ÿã€‚è®©æˆ‘ä»¬ä¸€èµ·åˆ›é€ å‡ºè‰²çš„ä½œå“ã€‚"
            ]
        }
    
    async def chat_completion(
        self, 
        request: ChatCompletionRequest, 
        model: Model
    ) -> ChatCompletionResponse:
        """Generate mock completion response"""
        import random
        
        # Get model configuration
        model_config = self.mock_responses.get(model.name, {
            "base": f"æˆ‘æ˜¯{model.name}ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼",
            "capabilities": ["é€šç”¨å¯¹è¯"],
            "response_time": (0.5, 2.0)
        })
        
        # Simulate realistic processing time
        response_time = random.uniform(*model_config["response_time"])
        await asyncio.sleep(response_time)
        
        # Analyze user input
        last_message = request.messages[-1].content if request.messages else ""
        message_lower = last_message.lower()
        
        # Generate contextual response
        response_content = self._generate_contextual_response(
            last_message, model_config, model.name
        )
        
        # Add conversation context awareness
        if len(request.messages) > 1:
            response_content = self._add_context_awareness(request.messages, response_content)
        
        # Calculate realistic token usage
        prompt_tokens = sum(len(msg.content) // 4 for msg in request.messages)  # Approximate tokenization
        completion_tokens = len(response_content) // 4
        total_tokens = prompt_tokens + completion_tokens
        
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            created=int(time.time()),
            model=model.name,
            choices=[
                ChatCompletionChoice(
                    index=0,
                    message=ChatMessage(
                        role=MessageRole.ASSISTANT,
                        content=response_content
                    ),
                    finish_reason="stop"
                )
            ],
            usage=ChatCompletionUsage(
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens
            )
        )
    
    async def chat_completion_stream(
        self, 
        request: ChatCompletionRequest, 
        model: Model
    ) -> AsyncGenerator[ChatCompletionChunk, None]:
        """Generate mock streaming response"""
        # Get the complete response first
        complete_response = await self.chat_completion(request, model)
        content = complete_response.choices[0].message.content
        
        # Split content into chunks for streaming
        words = content.split()
        chunk_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
        created = int(time.time())
        
        # Send initial chunk with role
        yield ChatCompletionChunk(
            id=chunk_id,
            created=created,
            model=model.name,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(
                        role=MessageRole.ASSISTANT
                    )
                )
            ]
        )
        
        # Send content chunks
        for i, word in enumerate(words):
            await asyncio.sleep(0.05)  # Simulate streaming delay
            
            content_chunk = word + (" " if i < len(words) - 1 else "")
            
            yield ChatCompletionChunk(
                id=chunk_id,
                created=created,
                model=model.name,
                choices=[
                    ChatCompletionChunkChoice(
                        index=0,
                        delta=ChatCompletionChunkDelta(
                            content=content_chunk
                        )
                    )
                ]
            )
        
        # Send final chunk with finish_reason
        yield ChatCompletionChunk(
            id=chunk_id,
            created=created,
            model=model.name,
            choices=[
                ChatCompletionChunkChoice(
                    index=0,
                    delta=ChatCompletionChunkDelta(),
                    finish_reason="stop"
                )
            ]
        )
    
    def _generate_contextual_response(self, user_message: str, model_config: dict, model_name: str) -> str:
        """Generate contextual response based on user input"""
        import random
        
        message_lower = user_message.lower()
        base_response = model_config["base"]
        
        # Greeting detection
        if any(word in message_lower for word in ["ä½ å¥½", "hello", "hi", "å—¨", "æ‚¨å¥½"]):
            return random.choice(self.response_templates["greeting"])
        
        # Programming detection
        if any(word in message_lower for word in ["ç¼–ç¨‹", "ä»£ç ", "programming", "code", "ç®—æ³•", "debug", "bug"]):
            if "ç¼–ç¨‹" in model_config["capabilities"] or "ä»£ç " in model_config["capabilities"]:
                return random.choice(self.response_templates["programming"])
            else:
                return f"{base_response}\n\nè™½ç„¶æˆ‘ä¸æ˜¯ä¸“é—¨çš„ç¼–ç¨‹æ¨¡å‹ï¼Œä½†æˆ‘ä»å¯ä»¥ä¸ºæ‚¨æä¾›åŸºç¡€çš„ç¼–ç¨‹å¸®åŠ©ã€‚"
        
        # Analysis detection
        if any(word in message_lower for word in ["åˆ†æ", "analyze", "æ•°æ®", "ç»Ÿè®¡", "è¶‹åŠ¿"]):
            return random.choice(self.response_templates["analysis"])
        
        # Creative detection
        if any(word in message_lower for word in ["åˆ›æ„", "creative", "å†™ä½œ", "æ•…äº‹", "è¯—æ­Œ", "è®¾è®¡"]):
            return random.choice(self.response_templates["creative"])
        
        # Time query
        if any(word in message_lower for word in ["æ—¶é—´", "time", "ç°åœ¨", "ä»Šå¤©", "æ—¥æœŸ"]):
            current_time = time.strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
            return f"{base_response}\n\nå½“å‰æ—¶é—´æ˜¯ï¼š{current_time}ã€‚éœ€è¦æˆ‘å¸®æ‚¨å¤„ç†ä¸æ—¶é—´ç›¸å…³çš„å…¶ä»–äº‹åŠ¡å—ï¼Ÿ"
        
        # Default intelligent response
        return self._generate_intelligent_response(user_message, model_config, model_name)
    
    def _generate_intelligent_response(self, user_message: str, model_config: dict, model_name: str) -> str:
        """Generate intelligent response for general queries"""
        import random
        
        responses = [
            f"æ ¹æ®æ‚¨çš„é—®é¢˜ã€Œ{user_message}ã€ï¼Œæˆ‘æ¥ä¸ºæ‚¨è¯¦ç»†åˆ†æå’Œè§£ç­”ã€‚\n\n{model_config['base']}\n\næˆ‘åœ¨{', '.join(model_config['capabilities'])}ç­‰æ–¹é¢å…·æœ‰ä¸“ä¸šèƒ½åŠ›ï¼Œå¯ä»¥ä¸ºæ‚¨æä¾›æ·±å…¥çš„å¸®åŠ©ã€‚",
            
            f"è¿™æ˜¯ä¸€ä¸ªå¾ˆæœ‰è¶£çš„é—®é¢˜ï¼{model_config['base']}\n\nå…³äºã€Œ{user_message}ã€ï¼Œæˆ‘å¯ä»¥ä»å¤šä¸ªè§’åº¦ä¸ºæ‚¨åˆ†æï¼š\n\n1. é—®é¢˜çš„æ ¸å¿ƒè¦ç‚¹\n2. å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ\n3. å®æ–½å»ºè®®\n\nè¯·å‘Šè¯‰æˆ‘æ‚¨æ›´å…³æ³¨å“ªä¸ªæ–¹é¢ï¼Ÿ",
            
            f"æ„Ÿè°¢æ‚¨çš„ä¿¡ä»»ï¼{model_config['base']}\n\né’ˆå¯¹æ‚¨æåˆ°çš„ã€Œ{user_message}ã€ï¼Œæˆ‘å»ºè®®æˆ‘ä»¬å¯ä»¥è¿™æ ·æ¥å¤„ç†ï¼š\n\nâ€¢ é¦–å…ˆç†è§£é—®é¢˜çš„èƒŒæ™¯å’Œç›®æ ‡\nâ€¢ ç„¶ååˆ†æå¯è¡Œçš„æ–¹æ³•\nâ€¢ æœ€åæä¾›å…·ä½“çš„å®æ–½æ­¥éª¤\n\næ‚¨è§‰å¾—è¿™ä¸ªæ€è·¯å¦‚ä½•ï¼Ÿ",
        ]
        
        return random.choice(responses)
    
    def _add_context_awareness(self, messages: list, response: str) -> str:
        """Add conversation context awareness"""
        if len(messages) > 3:
            return f"åŸºäºæˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œ{response}\n\næˆ‘æ³¨æ„åˆ°è¿™æ˜¯æˆ‘ä»¬ç¬¬{len(messages)//2}è½®å¯¹è¯äº†ï¼Œå¦‚æœæ‚¨æœ‰å…¶ä»–ç›¸å…³é—®é¢˜ï¼Œæˆ‘å¾ˆä¹æ„ç»§ç»­ä¸ºæ‚¨è§£ç­”ã€‚"
        elif len(messages) == 3:
            return f"ç»§ç»­æˆ‘ä»¬çš„è®¨è®ºï¼Œ{response}"
        else:
            return response